{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"row\">\n",
    "    <div class=\"column\">\n",
    "        <img src=\"https://datasciencecampus.ons.gov.uk/wp-content/uploads/sites/10/2017/03/data-science-campus-logo-new.svg\"\n",
    "             alt=\"Data Science Campus Logo\"\n",
    "             align=\"right\" \n",
    "             width = \"340\"\n",
    "             style=\"margin: 0px 60px\"\n",
    "             />\n",
    "    </div>\n",
    "    <div class=\"column\">\n",
    "        <img src=\"https://cdn.ons.gov.uk/assets/images/ons-logo/v2/ons-logo.svg\"\n",
    "             alt=\"ONS Logo\"\n",
    "             align=\"left\" \n",
    "             width = \"420\"\n",
    "             style=\"margin: 0px 30px\"/>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Python: Day 2\n",
    "\n",
    "## Trainers\n",
    "\n",
    "<font size=\"+0.5\">Saliha Minhas</font>   \n",
    "(<saliha.minhas@ons.gov.uk>)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "\n",
    "<br>\n",
    "<a href=\"#Learning Objectives\"><font size=\"+1\">Learning Objectives</font></a>\n",
    "\n",
    "\n",
    "<br>\n",
    "<b>Section 1: </b><a href=\"# The Pandas Library - Objects for Data Science\"><font size=\"+1\">The Pandas Library - Objects for Data Science</font></a>\n",
    "<br>\n",
    "<ol>\n",
    "  <li>General Purpose Objects</li>\n",
    "  <li>Series Objects</li>\n",
    "  <li>DataFrame Objects</li>\n",
    "  <li>Import Pandas</li>\n",
    "  <li>Reading in Data</li>\n",
    "  <li>Reading a CSV file</li>\n",
    "  <li>Getting Help</li>\n",
    "</ol>\n",
    "<br>\n",
    "\n",
    "\n",
    "<b>Section 2: </b><a href=\"#Selecting and Filtering DataFrames\"><font size=\"+1\">Selecting and Filtering DataFrames</font></a>\n",
    "<br>\n",
    "<ol>\n",
    "  <li>Selecting Columns from DataFrames</li>\n",
    "  <li>Filtering rows from Dataframes</li>\n",
    "  <li>Conditional Filtering</li>\n",
    "  <li>Cperators to filter data</li>\n",
    "  <li>Using Multiple Conditions to Filter</li>\n",
    "</ol>\n",
    "<br>\n",
    "\n",
    "<b>Section 3: </b><a href=\"#Generating New Variables\"><font size=\"+1\">Generating New Variables</font></a>\n",
    "<br>\n",
    "<ol>\n",
    "    <li>Creating Binary Variables</li>\n",
    "    <li>Constant Value Variables</li>\n",
    "    <li>Creating Variables Based on Existing Columns</li>\n",
    "    <li>Classifying Numerical Values using pd.cut() and pd.qcut()</li>\n",
    "    <li>Removing (Dropping) Columns</li>\n",
    "\n",
    "</ol>\n",
    "<br>\n",
    "\n",
    "<b>Section 4: </b><a href=\"# Descriptive Statistics\"><font size=\"+1\">Descriptive Statistics</font></a>\n",
    "<br>\n",
    "<ol>\n",
    "    <li>Describing Numerical Data</li>\n",
    "    <li>Descriptive Statistics for Numerical Data</li>\n",
    "    <li>Describing Text (or Categorical) Data</li>\n",
    "    <li>Sorting Data</li>\n",
    "</ol>\n",
    "<br>\n",
    "\n",
    "<b>Section 5:  </b><a href=\"#Updating-Values\"><font size=\"+1\">Updating Values</font></a>\n",
    "<br>\n",
    "<ol>\n",
    "    <li>Copies and Views</li>\n",
    "    <li>Selecting and Filtering with `.loc[row_indexer, col_indexer]` and `.iloc[row_indices, col_indices]`</li>\n",
    "    <li>Updating DataFrame Cells</li>\n",
    "    <li>Propagating Missing Data Values with .loc[]</li>\n",
    "    <li>Changing column data types</li>\n",
    "    <li>Changing column names</li>\n",
    "</ol>\n",
    "<br>\n",
    "\n",
    "<b>Section 6: </b><a href=\"#Aggregation\"><font size=\"+1\">Aggregation</font></a>\n",
    "<br>\n",
    "<ol>\n",
    "    <li>Aggregation using the `.groupby()` method</li>\n",
    "</ol>\n",
    "<br>\n",
    "\n",
    "\n",
    "<b>Section 7: </b><a href=\"#Crosstabs,-Contingency,-and-Two-Way-Tables\"><font size=\"+1\">Crosstabs, Contingency, and Two Way Tables</font></a>\n",
    "<br>\n",
    "<ol>\n",
    "     <li>Crosstabulation using the `pd.crosstab()` top-level function</li>\n",
    "</ol>\n",
    "<br>\n",
    "\n",
    "<b>Section 8: </b><a href=\"#Merging-Data\"><font size=\"+1\">Merging Data</font></a>\n",
    "<br>\n",
    "<ol>\n",
    "    <li>Different ways to merge</li>\n",
    "    <li>Merge function</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Learning Objectives\n",
    "\n",
    "\n",
    "The goal for today's session is develop a working knowledge of the Pandas DataFrame (and Series), including how to:\n",
    "\n",
    "* Select and Filter columns\n",
    "* Describe numeric and categorical data\n",
    "* Update values and columns\n",
    "* Aggregate and cross-tabulate data\n",
    "* Merge\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Section 1:</b> \n",
    "\n",
    "\n",
    "# The Pandas Library - Objects for Data Science\n",
    "\n",
    "\n",
    "\n",
    "## 1.1 General Purpose Objects\n",
    "\n",
    "The data structures - lists, tuples, and dictionaries, are the core, in-built data structures available in python. They are always available for use and easily created and destroyed. They are general purpose which means they are useful in many situations, however, they may not allow us to complete specific tasks as easily as we would like.\n",
    "\n",
    "Nonetheless, it is really important to know a bit about lists, tuples and dictionaries because they are used throughout python, and you will be very likely to encounter them in some form. Perhaps as an output from a function or method; as a way of providing input to a procedure; or, as the way that the library you are using stores properties.\n",
    "\n",
    "Many advanced python libraries that have been built for specific purposes, like data science, take advantage of the fact that objects are extensible. Programmers can create new classes based on existing objects and add in specific properties or methods that do specific things. This means that general purpose objects can be customised for particular tasks.\n",
    "\n",
    "In data science applications, the Pandas library provides specialised objects that make working with data tables a lot easier than if we had to rely on python's general purpose objects. However, because pandas objects are specialised, they are also more complicated than general purpose objects.\n",
    "\n",
    "Pandas adds two important data structures that we need to know about. The 1 dimensional `Series` and the 2 dimensional `DataFrame`.\n",
    "\n",
    "\n",
    "## 1.2 Series Objects\n",
    "\n",
    "* A Series is like a one-dimensional array (effectively a list) with a label for each observation.\n",
    "\n",
    "* You can think of a Series as being similar to a single column in a spreadsheet: a list of values.\n",
    "\n",
    "* However, the items in a Series must all be of the same data type, unlike a list.\n",
    "\n",
    "* The Series index provides a label for each observation, the default is for consecutive integer labels starting at 0.\n",
    "\n",
    "* However, a Series index does not have to be consecutive numbers, they can be non-unique numbers, non-consecutive numbers, as well as string objects or tuples.\n",
    "\n",
    "* As such, the Series itself is a bit like a list, whereas the Series index is a bit like a dictionary, and is able to return a value in the Series based on the index, like a key-value pair.\n",
    "\n",
    "* The Series object implements a large number of special behaviours (methods, procedures) that can be called on the data held in a Series to get useful outputs. This includes mathematical functions, like taking the average of all values in a numerical Series.\n",
    "\n",
    "## 1.3 DataFrame Objects\n",
    "\n",
    "* A DataFrame is a two-dimensional version of a Series object.\n",
    "\n",
    "* In effect, a DataFrame is a collection of Series objects - one Series for each column in the DataFrame.\n",
    "\n",
    "* A DataFrame is thus similar to a whole spreadsheet, like an Excel file, a STATA .dta file, a SAS file, an SPSS .sav file etc.\n",
    "\n",
    "* A DataFrame can hold columns with different data types, but as per Series above, any single column must contain values of the same data type.\n",
    "\n",
    "* The dimensions are labeled in a similar way to the Series object:\n",
    "\n",
    "    * **index** - refers to the row labels\n",
    "    * **columns** - refers to the column labels\n",
    "    \n",
    " * Having indexed data allows fast look up and powerful relational operations \n",
    " \n",
    " * Each row has a label and each column has a label\n",
    "\n",
    "Most the the time you'll be working with DataFrame objects in Pandas, however, as DataFrames are composed of Series, it is very likely that through indexing, selection, creation of new columns, and analysis you'll encounter Series objects too. Luckily, DataFrame and Series objects are similar, DataFrames effectively extend the Series into two-dimensions and as such implement some additional properties and methods that may not be relevant to Series on their own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Import Pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing pandas is this simple!\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code cell has two lines.\n",
    "\n",
    "The first line is a comment - Python (and other languages) \"skip\" anything with a # infront of it.\n",
    "\n",
    "On the second line I've written my import statement. There are two parts to this statement:\n",
    "1. `import pandas`\n",
    "2. `as pd`\n",
    "\n",
    "In python, it is sufficient just to write `import pandas`.\n",
    "\n",
    "`pd` is a common nickname or 'alias' used for pandas - rather than write 'pandas' each time I can just write 'pd.'\n",
    "\n",
    "Many packages have nicknames that are consistently applied by members of the wider python community, such as pandas being pd. You don't have to adhere to these conventions, but you'll probably see them used a lot in training and help materials online.\n",
    "\n",
    "When you successfully import a module, you don't usually get any feedback. However, if you get it wrong you'll get an error message that should give you some idea of what happened, for instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If you run this code cell you should get a 'ModuleNotFoundError' as there is no 'pandahs' module!\n",
    "import pandahs # by the way, I can also comment on the end of lines too!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Reading in Data\n",
    "\n",
    "Now that you've loaded pandas, lets read some data into python and see what that looks like.\n",
    "\n",
    "Pandas can read data in a variety of different formats. In the code cell below type pd.read then press **Tab**\n",
    "\n",
    "Pressing Tab gives you all the things you can do with pandas that start with 'read'. You probably recognise some of the data input types (e.g. Excel, SAS, Stata), while others you may not have heard of (e.g. pickle, json etc.) You can then select whichever method you wish to use.\n",
    "\n",
    "**Tab** is a really useful shortcut to know for finding or completing commands in notebooks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we can read in a large variety of data using pandas data readers.\n",
    "\n",
    "(NB There isn't a specific reader for SPSS files - it is possible to read .sav files, unfortunately you need another library called savReaderWriter at the moment.)\n",
    "\n",
    "## 1.6 Reading a CSV file\n",
    "\n",
    "Now that you've seen the different options, try reading a simple csv (comma separated value) dataset. \n",
    "\n",
    "To read a csv file, the only piece of information we absolutely need is the location of the file.\n",
    "\n",
    "The file is called 'titanic.csv' and is in the data folder.\n",
    "\n",
    "The code below demonstrates how you can read these data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the data using an absolute path\n",
    "titanic = pd.read_csv('C:/intro to Python/titanic.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Exploring the Data\n",
    "\n",
    "There are a number of variables within this dataset:\n",
    "* pclass = Passenger class of travel.\n",
    "* survived = 1 if the passenger survived the sinking, 0 if not.\n",
    "* name = Full name of the passenger, including title.\n",
    "* sex = Passenger gender.\n",
    "* age = Passenger age.\n",
    "* sibsp = Count of siblings or spouse also aboard.\n",
    "* Parch = Count of parents or children also aboard.\n",
    "* ticket = Ticket reference.\n",
    "* fare = fare paid.\n",
    "* cabin = Cabin number.\n",
    "* embarked = Port of embarkation. (S = Southampton (UK); C = Cherbourg (France); Q = Queenstown (Cobh, Ireland))\n",
    "\n",
    "\n",
    "\n",
    "The first thing we may want to do having read in a DataFrame is to take a quick overview and check it looks right.\n",
    "\n",
    "There are several DataFrame methods we might use to do this: `.head()`, `.tail()` and `.sample()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# head shows the first n rows of the dataframe (5 by default).\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tail shows the last n rows of a dataframe (5 by default)\n",
    "titanic.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sample shows n rows chosen at random from the dataframe (1 by default)\n",
    "titanic.sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dimensions of dataframe\n",
    "titanic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the number of rows and columns\n",
    "# NB This line 'unpacks' the tuple returned by shape\n",
    "numrows, numcols = titanic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print out the number of rows and columns\n",
    "print(\"There are {} rows and {} columns in the titanic dataset\".format(numrows, numcols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of rows using pythons inbuilt len() function (len meaning 'length')\n",
    "numrows = len(titanic)\n",
    "# print the number of rows using f-string formatting.\n",
    "print(f\"There are {numrows} rows in the titanic dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#type of object\n",
    "type(titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get column data types in dataframe\n",
    "titanic.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember our different data types - \n",
    "\n",
    "* **int** indicates integers, 'parch' for instance is recording counts as whole numbers, such as: 0, 1, 2 etc.\n",
    "* **float** indicates 'floating-point numbers', effectively decimal numbers like in the age column.\n",
    "* **object** indicates text, also known as 'string' data. The 'name' column gives passenger full names and titles.\n",
    "* **bool** which indicates Boolean values, Booleans encode True or False values.\n",
    "\n",
    "Other data types you might see include:\n",
    "* **datetime** which encodes date and time values.\n",
    "* **category** which is a special Pandas datatype for categorical or factor variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Column names are really easy to get!\n",
    "colnames = titanic.columns\n",
    "colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columnsNamesArr = titanic.columns.values\n",
    "columnsNamesArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# colnames is an index object, if we wanted a list we could use:\n",
    "#colnames = colnames.tolist()\n",
    "# or\n",
    "colnames = list(colnames)\n",
    "colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#DataFrames also have an `.info()` method which returns a concise summary of information about the Data.\n",
    "titanic.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the different counts in the information above, this suggests that some variables are completely observed (e.g. pclass, survived), while others have missing data values (e.g. age, embarked, cabin).\n",
    "\n",
    "When you read a DataFrame into pandas, the data are loaded in memory. This means that any changes you make won't be reflected in the original file you loaded. If you want to preserve the changes you make to the dataset you have to export the DataFrame object to a file.\n",
    "\n",
    "Pandas has a number of file writers. Let's save the `titanic` DataFrame as an excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# File writers are prefixed with .to_ press tab to find avaialble options.\n",
    "titanic.to_excel('../Save/titanic.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Getting Help\n",
    "\n",
    "Most python modules include information on how to use their functions with something called a 'docstring'. You can look at docstrings using python's in-built help() function. Run the code below to look at the docstring for the pandas read_csv function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Have a look at the docstring using python.\n",
    "help(pd.read_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you're using a notebook, you can also access the docstring by pressing Shift-Tab with the cursor somewhere in the object. This produces a **tool-tip**. A small pop-up box with relevant information.\n",
    "\n",
    "Pressing Tab once shows the parameters available for the particular object you are looking at.  \n",
    "Pressing Tab twice shows the whole docstring.  \n",
    "Pressing Tab a third time makes the tool-tip linger for 10 seconds.  \n",
    "Pressing Tab a fourth time put the tool-tip into a larger pane in the browser.  \n",
    "\n",
    "The tool-tip provides the same information as the `help()` function, but with nicer formatting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.read_d_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Docstrings**\n",
    "\n",
    "\n",
    "Docstrings look a bit scary at first, but once you learn how to read them they're super useful!\n",
    "\n",
    "The first block of text is the function we're looking at - read_csv - followed by all the possible parameters you can use to modify how pandas reads a csv file.\n",
    "\n",
    "Note that the first parameter is 'filepath_or_buffer', that's currently all you're passing to this function - the filepath of 'MarvelUniverse.csv'.\n",
    "\n",
    "If you scroll down a little, you can then see all of the possible parameters listed and described.\n",
    "\n",
    "E.g If your data were semi-colon delimited you'd have to specify the 'sep' parameter. \n",
    "No header row? (i.e. column names) you could set 'header' to None.\n",
    "\n",
    "At the very bottom of the docstring, you can see that read_csv function returns a 'DataFrame'.\n",
    "\n",
    "Don't get too hung up on docstrings for now though, as you become more experienced they'll begin to make more sense!\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<b>Section 2:</b> \n",
    "\n",
    "# Selecting and Filtering DataFrames\n",
    "\n",
    "Over the following sections, we will learn how to select and filter data using pandas DataFrames. This is one of the most useful and powerful features of pandas.  \n",
    "\n",
    "It is useful for a range of reasons, from simply cutting down a large dataset into the specific sub-sets of data required for analysis, to managing the various components of a model (e.g. dependent and independent variables, training and test data etc.) and conducting specific subgroup analyses.\n",
    "\n",
    "Selecting and filtering can be done by using the indexing operator. Pandas uses the same indexing operator as lists, tuples, and dictionaries - `[]` (square brackets).\n",
    "\n",
    "However, the DataFrame indexing operator is more sophisticated than one used for the python built-in data structures, the behaviour of the DataFrame indexer depends, as you'll see, on what you pass to the DataFrame indexing operator. This allows you to pass different kinds of information to the same indexing operator and get specific outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Selecting Columns from DataFrames\n",
    "\n",
    "The simplest way to select a column from a dataframe is to use the name of that column!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select by passing the name of a column as a string.\n",
    "passengers = titanic['name']\n",
    "passengers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, the following things happen:\n",
    "1. We index the DataFrame `titanic` using the column header.\n",
    "2. Assuming 'name' is a legitimate column header, a pandas Series is returned.\n",
    "3. This Series, representing the 'name' column of the `titanic` Dataframe, is assigned to a variable called `passengers`\n",
    "5. Finally, we look at the first 5 rows of the Series object `passengers`\n",
    "\n",
    "If we want to select multiple columns, we have to first collect the column names together using a list, and then pass that to the DataFrame indexing operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First make a list of column names called cols\n",
    "cols = ['name','age']\n",
    "# Use cols to select multiple columns from marvel.\n",
    "passenger_cols = titanic[cols]\n",
    "passenger_cols.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above is very similar to the code for selecting a single column, the main difference is that to select multiple columns we pass a list of string objects, rather that a single string object directly.\n",
    "\n",
    "However, because we have selected multiple columns, the `passenger_cols` object is actually a DataFrame, rather than a Series object.\n",
    "\n",
    "Note, that we don't have to create the `cols` list first, we can actually create it on-the-fly in the indexing operator, you just have to learn to distinguish the list constructor square brackets from the indexing square brackets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select multiple columns directly.\n",
    "passenger_cols = titanic[['name','age']]\n",
    "passenger_cols.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Just have a look, rather than assigning to a variable\n",
    "titanic[['name','age']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "1. Refresh your memory of the titanic data by getting:\n",
    "    * The number of rows and columns in the DataFrame\n",
    "    * The datatypes of the columns.\n",
    "    * A list of the columns names for the titanic dataset.\n",
    "2. Select the 'fare' column from the `titanic` data and show the tail of the data.\n",
    "3. Select just the last column, try using the list of column names you made earlier.\n",
    "4. Select the second, third and fourth columns, try doing it using DataFrame columns property directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Filtering rows from Dataframes\n",
    "\n",
    "Filtering rows from a pandas dataframe works in a very similar way to selecting columns. Simple filtering can be achieved by passing a range to the DataFrame indexer, just like slicing a list.\n",
    "\n",
    "The code below does the same thing as `.head()` and `.tail()` and can be used to show any arbitrary range of rows in a given DataFrame.\n",
    "\n",
    "Note that this is identical to slicing a list.\n",
    "\n",
    "However, we can't get individual rows by indexing as we would with a list, because a column could be named with an integer. This would mean that `dataframe[0]` is ambiguous and could refer to the first row, or a column named 0. Hence it is not allowed, `dataframe[0]` only works if you have a column named '0', which is a default for some operations in pandas.\n",
    "\n",
    "This means that selecting a single row also requires a slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first 5 rows\n",
    "titanic[0:5] # or - titanic[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# last 5 rows\n",
    "titanic[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# arbitrary slice\n",
    "titanic[102:109]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1 row\n",
    "titanic[123:124]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Conditional Filtering\n",
    "\n",
    "Most filtering is a more involved operation where we first specify the condition(s) that must be met in order for rows to be included in or excluded from the output dataframe.\n",
    "\n",
    "The way that pandas filters rows can be though of as a two-step process.\n",
    "\n",
    "1. Create a 'mask' that specifies inclusion and exclusion for each row in the dataframe.\n",
    "2. Mask the dataframe to return the subset of rows that are included.\n",
    "\n",
    "This sounds a bit abstract, so let's consider what this might look like in practice.\n",
    "\n",
    "Imagine you have the following (very simple) dataframe, called 'catdog':\n",
    "\n",
    "index | Animal | Name\n",
    "---| --- | ---\n",
    "0 | Cat | Catalie Portman\n",
    "1 | Cat | Pico de Gato\n",
    "2 | Dog | Chewbarka\n",
    "3 | Cat | JK Meowling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want to filter so you just have 'Cat' rows. Therefore you design the following condition:\n",
    "\n",
    "```python \n",
    "mask = catdog['Animal'] == 'Cat'\n",
    "```\n",
    "\n",
    "There are a lot of = (equals) in the above statement.\n",
    "* The first = indicates assignment, we are assigning the outcome of the expression on the right to the variable on the left of the equals sign.\n",
    "* The second double equals sign, ==, indicates a comparison, in this case it assesses whether each value in the 'Animal' column of catdog is equal to the text 'Cat'. If python finds that the column value and 'Cat' are the same it assigns a True value, and if not a False value.\n",
    "\n",
    "This produces a 'mask' which is a Series of `True` and `False` values against the DataFrame index.\n",
    "\n",
    "index | &#xfeff;\n",
    "---|---\n",
    "0 | True\n",
    "1 | True\n",
    "2 | False\n",
    "3 | True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you just have to pass the mask to the original dataframe to complete the filtering process.\n",
    "\n",
    "```python\n",
    "catdog2 = catdog[mask]\n",
    "catdog2\n",
    "```\n",
    "This subsets the catdog dataframe based on the True (include) and False (exclude) values. Producing:\n",
    "\n",
    "index | Animal | Name\n",
    "---| --- | ---\n",
    "0 | Cat | Catalie Portman\n",
    "1 | Cat | Pico de Gato\n",
    "3 | Cat | JK Meowling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The row that had a 'Dog' value for 'Animal', has been removed. Note though that the index has remained the same as the original. Sometimes it is important to reset the index after filtering to restore the index to sequential integers starting at 0.\n",
    "\n",
    "If you want to reset the index - you can do so by using this code - \n",
    "\n",
    "```python\n",
    "catdog2 = catdog2.reset_index(drop = True)\n",
    "catdog2\n",
    "```\n",
    "index | Animal | Name\n",
    "---| --- | ---\n",
    "0 | Cat | Catalie Portman\n",
    "1 | Cat | Pico de Gato\n",
    "2 | Cat | JK Meowling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See above that the index has been reset to be sequential.\n",
    "\n",
    "## 2.4 Operators to filter data\n",
    "\n",
    "We can filter by using logical comparison statements\n",
    "\n",
    "* == 'is equal to' notice the double == and watch out! A single one would be assigning to a variable!\n",
    "* != 'does not equal' - the opposite of ==\n",
    "* $\\gt$  greater than\n",
    "* $\\lt$ less than\n",
    "* $\\gt$= greater than or equal too\n",
    "* $\\lt$= less than or equal too.\n",
    "\n",
    "In addition, pandas includes some functions to make particular comparisons easier:\n",
    "\n",
    "* .isin(list) which we can use for multiple conditions \n",
    "* .between() which we can use to specify upper and lower bounds\n",
    "\n",
    "Finally, the ~ (tilde) allows us to flip or invert an expression. Basically, if an expression returns [true, true, false], the same expression with a ~ in front of it will return [false, false, true].\n",
    "\n",
    "However, we'll concentrate on the simple operators in the top list for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filter titanic for 3rd class passengers only\n",
    "\n",
    "# First make the mask\n",
    "mask = titanic['pclass'] == 3\n",
    "# Have a quick look at the mask\n",
    "mask.sample(5) # 5 rows in the mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now filter the titanic dataframe with this mask\n",
    "thirdclass = titanic[mask]\n",
    "thirdclass.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the same approach for other logical statements.\n",
    "mask = titanic['fare'] > 200\n",
    "titanic[mask].head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "1. Show the row for the passenger named: 'Birkeland, Mr. Hans Martin Monsen'\n",
    "2. How many passengers in the dataset are male?\n",
    "3. How many passengers are under 18 years of age?\n",
    "4. What proportion of passenger in the dataset survived?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Using Multiple Conditions to Filter\n",
    "\n",
    "So far, we've only filtered according to individual conditions set on a single column, but there is no reason we can't use multiple conditions to filter by several conditions and/or columns at once. However, we do need to think about how the conditions relate to each other, we have two options to establish these relationships.\n",
    "\n",
    "* **and** relationships are given by the **&** (ampersand) symbol. This implies both/all conditions must be met for a row to evaluate to True.\n",
    "* **or** relationships are given by the **|** (pipe) symbol. This implies that if _any_ of the conditions can be met a given row evaluates to True.\n",
    "\n",
    "You can think of `.isin()` and `.between()` as being special versions of multiple condition filters.\n",
    "\n",
    "* isin() is basically just a lot of linked **or** statements - *value1* **or** *value2* **or** *value3* etc.\n",
    "* between() is an **and** condition - greater than (or equal to) the lower bound **and** less than (or equal to) the upper bound.\n",
    "\n",
    "Let's again take a simple example to illustrate this with the `catdog` dataframe:\n",
    "\n",
    "index | Animal | Name | Age\n",
    "---| --- | --- | ---\n",
    "0 | Cat | Catalie Portman | 3.0\n",
    "1 | Cat | Pico de Gato | 5.0\n",
    "2 | Dog | Chewbarka | 1.0\n",
    "3 | Cat | JK Meowling | 7.0\n",
    "4 | Dog | K-9 | 11.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wanted to select all animals that are cats **and** who are over 4 years old, you could do the following:\n",
    "\n",
    "```python\n",
    "mask = (catdog['Animal'] == 'Cat') & (catdog['Age'] > 4.0)\n",
    "\n",
    "catdog[mask]\n",
    "```\n",
    "index | Animal | Name | Age\n",
    "---| --- | --- | ---\n",
    "1 | Cat | Pico de Gato | 5.0\n",
    "3 | Cat | JK Meowling | 7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only Cats over 4 years old have been included in the filter.\n",
    "\n",
    "However, if you wanted to select all animals that are either cats **or** are over 4 years old, you could instead do:\n",
    "\n",
    "```python\n",
    "mask = (catdog['Animal'] == 'Cat') | (catdog['Age'] > 4.0)\n",
    "\n",
    "catdog[mask]\n",
    "```\n",
    "index | Animal | Name | Age\n",
    "---| --- | --- | ---\n",
    "0 | Cat | Catalie Portman | 3.0\n",
    "1 | Cat | Pico de Gato | 5.0\n",
    "3 | Cat | JK Meowling | 7.0\n",
    "4 | Dog | K-9 | 11.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's try some multiple condition filters with the titanic data\n",
    "# First class passengers who are women.\n",
    "mask = (titanic['pclass'] == 1) & (titanic['sex']== 'female')\n",
    "titanic[mask].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Women or children\n",
    "mask = (titanic['sex'] == 'female') | (titanic['age'] < 18)\n",
    "titanic[mask].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try the special functions for multiple selection. First .isin()\n",
    "# Passeners from Cherbourg ('C') or Queenstown ('Q')\n",
    "titanic[titanic['embarked'].isin(['C','Q'])].sample(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now, .between()\n",
    "# passengers who paid between 100 and 250\n",
    "titanic[titanic['fare'].between(100,250)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "1. Select passengers who are in classes 2 and 3, what percentage of passengers is this?\n",
    "2. How many passengers who do not have siblings or spouses ('sibsp'), or parents or children ('parch') on the boat?\n",
    "3. What proportion of passengers who 'embarked' in Cherbourg ('C') or Queenstown ('Q') survived? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<b>Section 3:</b> \n",
    "\n",
    "\n",
    "# Generating New Variables\n",
    "\n",
    "There are a number of approaches to adding new columns of data to a DataFrame, and depending on what you want to achieve some of these can be quite complicated. We start with simple examples and build up to more sophisticated approaches later.\n",
    "\n",
    "## 3.1 Creating Binary Variables\n",
    "\n",
    "\n",
    "We can assign a condition to a new column to create a binary variable in much the same way as we might create a mask for filtering rows.\n",
    "\n",
    "Imagine that instead of filtering rows, we instead wanted to assign a 1 or a 0 to a new column depending on whether that condition was met. We can do this quite simply due to the fact that in python (and many other languages) a `True` value is equivalent to 1, and a `False` value is equivalent to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First create the condition as a True/False Series\n",
    "cond = titanic['sex'] == 'female'\n",
    "\n",
    "# Now assign the condition variable cond to a new column, but as an integer type.\n",
    "titanic['female'] = cond.astype(int)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, a condition is specified, returing a boolean Series. This Series object is converted to an integer data type and assigned to a new column in the `titanic` DataFrame called 'female'. If 'female' already existed, this code would have overwritten whatever was already in runtime.\n",
    "\n",
    "Using a condition we can either store `True` and `False` values directly, or convert them to their integer representations `1` and `0`. If we wanted to use arbitrary values for our new variable we can create a dictionary and `.map()` the dictionary to the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use YES and NO instead of True and False.\n",
    "titanic['female'] = cond.map({True:'YES',False:'NO'})\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, this `.map()` approach is also good for aggregating (dissolving) or recoding categorical variables. The dictionary can be of an arbitrary length and act as a lookup, this is a benefit of the key:value structure. Note though that pandas also implements its own `category` variable. We're not going to discuss it here as it's not strictly necessary, but it can be useful. Check the [pandas docs](https://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Constant Value Variables\n",
    "\n",
    "Constant values can also be assigned to all rows in a DataFrame with a single number or string, with the data type being dictated by the format of the value being assigned. This may be useful in the context of updating cells, which we'll discuss later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# New column of ints\n",
    "titanic['int_zeroes'] = 0\n",
    "# New column of floats\n",
    "titanic['float_ones'] = 1.0 # note floating point.\n",
    "# New column of strings\n",
    "titanic['string_twos'] = 'two'\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Creating Variables Based on Existing Columns\n",
    "\n",
    "We can simply assign the values of existing columns to new columns using assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic['name2'] = titanic['name']\n",
    "titanic.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use mathematical expressions with one or more existing columns to create new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The + 1 constant accounts for the passenger themself.\n",
    "titanic['family_size'] = titanic['sibsp'] + titanic['parch'] + 1\n",
    "titanic.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Classifying Numerical Values using pd.cut() and pd.qcut()\n",
    "\n",
    "New columns can be created directly when classifying numerical data using `pd.cut()` or `pd.qcut()`\n",
    "\n",
    "`cut()` has two behaviours. The default is to create a given number of equal-sized bins (e.g. the width of all bins are the same), however if you provide bins it will cut according to those bins.\n",
    "\n",
    "`qcut()` is similar, but rather than equal-sized bins it created bins that have (roughly) equal numbers of observations in them. These bins could have very different widths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Divide fare into 3 equally sized classes.\n",
    "titanic['fare_3class'] = pd.cut(titanic['fare'], 3, labels=['low','mid','high'])\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Divide fare into tertile.\n",
    "titanic['fare_tertiles'] = pd.qcut(titanic['fare'], 3, labels=['low','mid','high'])\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Removing (Dropping) Columns\n",
    "\n",
    "Not withstanding the fact that we could select the specific columns we want and exclude ones we don't want in our dataset, we also have a couple of options for dropping or deleting a column from a pandas DataFrame.\n",
    "\n",
    "Let's delete the constant valued columns we established earlier: 'int_zeroes', 'float_ones', 'string_twos'.\n",
    "\n",
    "The first option we have is the built in python statement `del`. Otherwise we can use the `.drop()` method.\n",
    "\n",
    "Note, that pandas is in active development, so sometimes parameters change as the library matures. If your `.drop()` method doesn't understand the code below then it may be an older version. try this instead:\n",
    "```python\n",
    "titanic.drop(['int_zeroes','float_ones','string_twos'], axis=1, inplace=True)\n",
    "```\n",
    "Note that this syntax is still compatible with newer versions of pandas, so old code won't break in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop a column with del\n",
    "del titanic['int_zeroes']\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop using the drop method\n",
    "titanic.drop(columns=['float_ones','string_twos'], inplace = True)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "1. Create a new binary variable called 'child', it should have the value 1 when the passenger is under 18 and 0 otherwise.\n",
    "2. Create a new variable called 'embarked_city', map 'S' to 'Southampton', 'C' to 'Cherbourg', and 'Q' to 'Queenstown'.\n",
    "3. Create a new variable called 'surname', the value should be the surname part of the 'name' field.\n",
    "    * Use `titanic['name'].str.split(',',expand=True)[0]` to get surnames.\n",
    "    * Explore this code and make sure you understand what's going on.\n",
    "    * How many unique surnames are there (hint: try `.unique()` or `.nunique()` on the new column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In that last question we made use of a special module on the `Series` object called `str`, this exposes all the string methods we've encountered previously to a column of string data, but in a vectorised form. This means you can manipulate text in a row-by-row manner with a single method call. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lower case names\n",
    "titanic['name'].str.lower().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Press tab after the last fullstop to see the string methods available.\n",
    "# Select one and use shift-tab to explore it further.\n",
    "pd.Series.str."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# in this example we are trying to extract all the titles from the names field.\n",
    "titanic['name'].str.split(',',expand=True)[1].str.split('.',expand=True)[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<b>Section 4:</b> \n",
    "\n",
    "\n",
    "# Descriptive Statistics\n",
    "\n",
    "There are really two broad types of data in our DataFrames at the moment that we want to look at - numerical data (i.e. ints and floats) and text data (i.e. strings; slightly confusingly called objects).\n",
    "\n",
    "In this section, we will explore some basic univariate descriptive statistics.\n",
    "\n",
    "## 4.1 Describing Numerical Data\n",
    "Let's start with the numerical data, because thats the easiest to work with. Pandas even has a built in function called `.describe()` which will provide some descriptive statistics for all the numerical columns in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Describe the titanic dataframe\n",
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it happens, the descriptive statistics output for our titanic dataset is also a DataFrame!\n",
    "\n",
    "Make sure you understand what each row means in this table:\n",
    "* **count** - the number (count) of entries in the given column.\n",
    "* **mean** - the average (arithmetic mean) data value in the given column.\n",
    "* **std** - the standard deviation (spread) of values in the given column.\n",
    "* **min** - the smallest value in the given column.\n",
    "* **25%** - the value of the data at the lower quartile (i.e. after the first 25% of data, ordered from smallest to largest).\n",
    "* **50%** - the middle value of the data (aka the median), half the values are larger than this value, and half smaller.\n",
    "* **75%** - the value of the data at the upper quartile (i.e. after the first 75% of data, ordered from smallest to largest).\n",
    "* **max** - the maximum data value recorded.\n",
    "\n",
    "We can get a sense of the data from these descriptive statistics. For instance: \n",
    "\n",
    "## 4.2 Descriptive Statistics for Numerical Data\n",
    "\n",
    "`.describe()` is great to get an overview, but what if we just wanted particular statistics and not the whole lot?\n",
    "\n",
    "Well, pandas will let you run a range of statistics individually! Some examples are given in the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count() can be defined for all datatypes, so all columns are computed. Note which columns have some missing data.\n",
    "titanic.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mean() is only defined for numeric columns\n",
    "titanic.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# std() is also only defined for numeric columns\n",
    "titanic.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# min() has a definition for numeric and text data.\n",
    "# The minimum value of a text field is the text which is first alphabetically.\n",
    "titanic.min() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# max() has a definition for numeric and text data.\n",
    "# The maximum value of a text field is the text which is last alphabetically.\n",
    "titanic.max() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# quantile() allows you to specify quantiles, such as 0.25 (lower quartile), 0.5 (median), and 0.75 (upper quartile)\n",
    "# for convenience median() also exists\n",
    "titanic.quantile(0.25) # 25% - lower quartile. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sum() works to concatenate text, producing a curious output.\n",
    "titanic.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hopefully though it is obvious that these methods could be called on selected columns too.\n",
    "titanic['fare'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# As it happens, python has a built-in sum, min and max functions which does the same thing.\n",
    "# however, pandas sum is better when confronted with missing data:\n",
    "sum(titanic['fare'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try this instead\n",
    "sum(titanic[titanic['fare'].notnull()]['fare'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell, a new filter condition for working with missing data is apparent: `notnull()` this returns `True` for rows that have a valid value, and `False` otherwise. Similar to the behaviour of `bool()`. The opposite of `notnull()` is `isnull()`.\n",
    "\n",
    "This method of selection is similar to making conditional statements with object methods that return a Boolean, e.g.\n",
    "```python\n",
    "if string_variable.islower():\n",
    "    # Do something\n",
    "```\n",
    "The same principle can apply to other contexts, for instance the `Series` object has a large number of string methods collected as `.str.`, calling `titanic['name'].str.contains('Mr.', regex=False)` returns `True` or `False` for each row in a column depending on whether it contains the substring 'Mr.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select passengers with title Mr. and get mean fare\n",
    "titanic[titanic['name'].str.contains('Mr.', regex=False)]['fare'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select passengers with title Mrs. and get mean fare\n",
    "titanic[titanic['name'].str.contains('Mrs.', regex=False)]['fare'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Describing Text (or Categorical) Data\n",
    "\n",
    "We can still use `.describe()` to look at text data, however we need to specify that we're looking at object (text) data types.\n",
    "\n",
    "Really, the descriptive statistics below are for categorical data, they don't work very well if every value in a field is a different piece of text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In the describe parameters we're only choosing to include object datatypes, given by 'O'.\n",
    "# The 'O' is in a list, because we could include other data types in the list if we wanted to.\n",
    "titanic.describe(include=['O'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are describing an object you get some different summary statistics than with numerical data:\n",
    "\n",
    "* **count** as before, a count of the values present in each column.\n",
    "* **unique** a count of the number of unique values in each column.\n",
    "* **top** is the most common value - aka the mode.\n",
    "* **freq** is the frequency of occurance of the most common value.\n",
    "\n",
    "Let's dig a bit deeper into some of these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Interestingly there are 2 James Kellys, however they don't appear to be duplicates.\n",
    "titanic[titanic['name'] == 'Kelly, Mr. James']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One way we could check for other name duplicates is by taking the mode.\n",
    "# As mode can be non-unique it returns a series\n",
    "# Looks like Kate Connolly is another possible duplicate.\n",
    "titanic['name'].mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Again, these appear to be different people!\n",
    "titanic[titanic['name'] == 'Connolly, Miss. Kate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the unique() function will give us all the unique objects in a column.\n",
    "titanic['embarked_city'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the value_counts() function gives a count for each unique value in a chosen column.\n",
    "titanic['embarked_city'].value_counts()\n",
    "# Most people embarked in Southampton."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Sorting Data\n",
    "\n",
    "Sorting data is straightforward in pandas, a simple sort on one columns used the DataFrame method `.sort_values()`:\n",
    "```python\n",
    "titanic.sort_values('age')\n",
    "```\n",
    "The default is to sort in ascending order, from smallest to largest value. Set the ascending parameter to `False` for a descending sort:\n",
    "```python\n",
    "titanic.sort_values('age', ascending = False)\n",
    "```\n",
    "This approach sorts and returns the entire DataFrame, if you want to sort a single column on its works similarly:\n",
    "```python\n",
    "titanic['age'].sort_values()\n",
    "```\n",
    "More complicated sorting behaviours can be managed by passing a list, in the order you would like the sort to occur:\n",
    "```python\n",
    "titanic.sort_values(['pclass','age'], ascending = [True, False])\n",
    "```\n",
    "In the above code I sort first by 'pclass' then by 'age'. in addition I pass a list to ascending indicating that 'pclass' is to be sorted in ascending order, and 'age' in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sort fare descending\n",
    "titanic.sort_values('fare', ascending = False).head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sort by sex ascending, then age descending\n",
    "titanic.sort_values(['sex','age'], ascending = [True, False]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sort by sex descending, then age descending\n",
    "titanic.sort_values(['sex','age'], ascending = False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5\n",
    "\n",
    "1. How old is the oldest passenger in the dataset?\n",
    "2. How many men and women are in the dataset?\n",
    "    * Check the pd.Series.value_counts() docstring and figure out how to get proportions of men and women.\n",
    "3. Create a new column called 'std_fare' which is the 'fare' minus the mean fare, divided by the standard deviation.\n",
    "4. Calculate the number of children in second class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Type your code here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solutions for exercise 5\n",
    "%load ../Solutions/Intro/exercise5.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<b>Section 5:</b> \n",
    "\n",
    "\n",
    "# Updating Values\n",
    "\n",
    "\n",
    "## 5.1 Important! Copies and Views\n",
    "\n",
    "The selecting of columns and filtering of rows that you've done above effectively **copies** the original dataframe to make a new one based on your conditions. This is great for creating a pared-down dataframe to work with, or when filtering data to produce statistics for particular subsets of data.\n",
    "\n",
    "However, when we want to actually update cell values we need to use a special function that ensures we are editing the original dataframe **view** and not a **copy** of it. If we edit cells on a copy of a dataframe, we might find that these values are not updated in the original dataframe when we come to analyse it!\n",
    "\n",
    "This is a technical point that you don't need to worry too much about, just remember to use the following approaches when dealing with code!\n",
    "\n",
    "In the titanic data, given what we know so far, we might try to update a cell values using what's called 'chained indexing':\n",
    "\n",
    "```python\n",
    "titanic[titanic['embarked'] == 'C']['embarked'] = 'c'\n",
    "```\n",
    "The above code makes sense - filter the rows, select the column and assign the value you want. However, python will give you a warning (specifically a 'SettingWithCopyWarning') that you're not doing things properly!\n",
    "\n",
    "Instead we'll do the following:\n",
    "\n",
    "```python\n",
    "titanic.loc[titanic['embarked'] == 'C','embarked'] = 'c'\n",
    "```\n",
    "\n",
    "The code looks very similar, however in the second (correct) example we're using `.loc[]`\n",
    "\n",
    "`.loc[]` is actually almost identical to the selecting and filtering we've already done. We just specify the rows and columns within the square bracket like: `.loc[row_indexer, col_indexer]` where:\n",
    "* `row_indexer` is the mask if we are filtering, or a colon, `:`, if we want to include all rows.\n",
    "* `col_indexer` is a single column name, or a list of column names, or a colon, `:`, if we want to include all columns.\n",
    "\n",
    "You can actually use the .loc[] approach to do all the selecting and filtering we've already done if you want.\n",
    "\n",
    "In addition to `.loc[]` we also have `.iloc[]` which behaves very similarly, except that it allows us to index on the index position of rows and columns.\n",
    "\n",
    "Some examples using iloc[]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select all rows, and the first column\n",
    "titanic.iloc[:,0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Or select a range of columns\n",
    "titanic.iloc[:,3:5].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Another head-like indexing approach\n",
    "titanic.iloc[0:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now some examples using `.loc[]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Often researchers use column selection to reorder the columns in a DataFrame.\n",
    "cols = list(titanic.columns)\n",
    "cols.reverse()\n",
    "titanic.loc[:,cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loc takes row conditions as usual.\n",
    "titanic.loc[titanic['pclass'].isin([1,2]),['pclass','name']].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often pandas users will use chained indexing when selecting and filtering, and the `loc` and `iloc` operators when cleaning values on a cell-by-cell basis. There is no reason why you can't use these operators for selecting and filtering too, however, if you want to assign the sub-set DataFrame to a new variable you'll need to use the `.copy()` method to ensure that it is a different DataFrame in memory, rather than a reference to the original.\n",
    "```python\n",
    "new_df = titanic.loc[:,['survived','embarked']].copy()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Propagating Missing Data Values with `.loc[]`\n",
    "\n",
    "Earlier, when we created a new binary variable called 'child' the condition we used created a `Series` of `True` and `False` values based on whether the condition was met or not.\n",
    "\n",
    "An important consideration when creating a new variable is presence of missing data. A condition will automatically set a missing value to `False`. This is fine for filtering data, but misrepresents data in a new variable - it makes a derived variable appear more complete than it is in reality.\n",
    "\n",
    "In order to update the 'child' column to incorporate rows we know are missing we can use `.loc[]`:\n",
    "```python\n",
    "# This was the column originally generated.\n",
    "titanic['child'] = (titanic['age'] < 18).astype(int)\n",
    "# Now, update the missing data values\n",
    "titanic.loc[titanic['age'].isnull(),'child'] = titanic[titanic['age'].isnull()]['age']\n",
    "```\n",
    "In the 2nd line of code, we use the row filter `titanic['age'].isnull()` to filter missing data in the 'age' column and we select the 'child' column. These are used to create a 'view' of the titanic DataFrame using `.loc[]`.\n",
    "\n",
    "Then, we assign to that combination of rows and columns the values given by: `titanic[titanic['age'].isnull()]['age']` which will be a Series of missing values. We should now see missing values in the 'child' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initial composition of 'child'\n",
    "titanic['child'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# update and check new field\n",
    "titanic.loc[titanic['age'].isnull(),'child'] = titanic[titanic['age'].isnull()]['age']\n",
    "titanic['child'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Changing Column Data Types\n",
    "\n",
    "As with basic python data types, we can cast columns of data from one data type to another. This can be useful as part of a data cleaning process. Sometimes we find that data we expect to be numeric is actually string data, often occurs because the numbers are actually stored as text in the original dataset, in which case they have to be converted. Other times it is because the original dataset includes characters that pandas won't immediately interpret as a numeric value. This is particularly the case if a dataset contains missing data that is coded to a special character. Reading in these data as text is the safest option, as it preserves all of the information and requires the data analyst to make a decision as to how to handle the conversion to a numeric data type.\n",
    "\n",
    "There are two ways to change a datatype, firstly the Series method: `.astype()` in which the parameter is a data type e.g.\n",
    "```python\n",
    "titanic['survived'].astype(str)\n",
    "```\n",
    "There is also a 'top-level' pandas function `pd.to_numeric()` which is a good option for data cleaning.\n",
    "```python\n",
    "pd.to_numeric(titanic['survived'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turned survived to a string column\n",
    "titanic['survived'] = titanic['survived'].astype(str)\n",
    "titanic.dtypes['survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turned survived back to a numeric\n",
    "titanic['survived'] = pd.to_numeric(titanic['survived'])\n",
    "titanic.dtypes['survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Updating Column Names\n",
    "\n",
    "We've been using a dataset that has good, descriptive column names. However, sometimes you're faced with columns that have difficult to use names.\n",
    "\n",
    "* column names might be really long, these a are a pain to type out.\n",
    "* spaces between words, and presence of special characters (e.g. %^& etc.) can be annoying\n",
    "* columns names may be ambiguous or not sufficiently descriptive.\n",
    "\n",
    "We can use the pandas `.rename()` method to update column names. To update the column names we pass a dicitonary to the parameter 'columns', e.g.\n",
    "```python\n",
    "df.rename(columns={'two':'new_name'}, inplace=True)\n",
    "```\n",
    "For a DataFrame `df`, the `rename` method allows us to change the names of columns based upon a dictionary. Here, the dictionary key `'two'` is the current name of the column, and the dictionary value `'new_name'` is what we want to rename the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic.rename(columns={'surname':'family_name'}, inplace=True)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<b>Section 6:</b> \n",
    "\n",
    "\n",
    "# Aggregation\n",
    "\n",
    "Aggregation means grouping data together by a particular grouping variable and producing a summary of one or more columns for that grouping variable.\n",
    "\n",
    "## 6.1 Aggregation using the .groupby() method\n",
    "\n",
    "We'll use the `groupby()` function. \n",
    "\n",
    "This function can be really useful, especially when your data are disaggregate - e.g. data about individual units of people or things. \n",
    "\n",
    "`groupby()` allows you to aggregate by a categorical variable and summarise numerical data into a new dataframe.\n",
    "\n",
    "`.groupby()` works on a principle known as 'split-apply-combine':\n",
    "* Split - a dataframe is divided into a set of smaller dataframes based on the grouping variable.\n",
    "* Apply - an aggregation is applied to each of the groups to create a single row for each group in the original dataframe.\n",
    "* Combine - bring together the aggregated dataframe rows into a final new dataframe.\n",
    "\n",
    "Let's walk through what that might look like for the `titanic` dataframe:\n",
    "* Firstly, we decide to **split** the data by the 'pclass'. This divides the `titanic` dataframe into effectively three separate dataframes, one for first, one for second and one for third class.\n",
    "* Secondly, we **apply** an aggregation to the dataframe. You can either produce an aggregate statistic for all rows, or you can selected specific columns on which to do the aggregation. If we **apply** a `.mean()` aggregation to 'fare', then for each 'pclass' group we get the average fare cost.\n",
    "* Finally, pandas returns a **combined** dataframe that contains the new aggregate statistics.\n",
    "\n",
    "Let's look at that in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic.groupby('pclass')['fare'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Similarly a count of children by class might look like this:\n",
    "titanic.groupby('pclass')['child'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully this all sounds fairly straightforward! `.groupby()` is a powerful tool, particularly when you are working with any kind of hierarchical data where you might want to know something aggregate about the groups within the data, for instance:\n",
    "* individuals nested in households.\n",
    "* employees nested in firms.\n",
    "* patients nested in primary or secondary care trusts.\n",
    "* small area geographies (e.g. wards, output areas, postcodes etc.) nested in larger geographies (e.g. districts, counties etc.)\n",
    "* countries nested in supra-national entities.\n",
    "\n",
    "or, demographic, cultural and socio-economic classes:\n",
    "* individuals by age, sex, ethnicity, religion etc.\n",
    "* employees by grade or occupational social class.\n",
    "* households by neighbourhood deprivation rank or decile.\n",
    "* experimental subjects in intervention and control arms of a trial.\n",
    "\n",
    "We can also aggregate according to more complicated groupings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Groupby passenger class, then city of embarkation.\n",
    "titanic.groupby(['pclass','embarked_city'])['fare'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NB order is important to the output.\n",
    "titanic.groupby(['embarked_city','pclass'])['fare'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ordering of groups may be important as it affects the resultant DataFrame.\n",
    "\n",
    "If you assign the `groupby()` output to a variable, you can also pull out dataframes for particular groups, just as if you had written a filter condition!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = titanic.groupby('pclass')\n",
    "classes.get_group(3).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<b>Section 7:</b> \n",
    "\n",
    "\n",
    "# Crosstabs, Contingency, and Two-Way Tables\n",
    "\n",
    "Similar to aggregation with `.groupby()` a cross-tabulation table allows you to generate frequencies for combinations of groups of data.\n",
    "\n",
    "## 7.1 Crosstabulation using the `pd.crosstab()` top-level function\n",
    "\n",
    "Crosstabs are often also refered to as 'contingency tables' and 'two-way tables' and, although there may be some subtle distinctions, these all refer to the setting of one categorical variable against another, creating a matrix, and then counting, or otherwise aggregating by cell values.\n",
    "\n",
    "To create cross-tabulations, you can use the `pandas.crosstab()` function.\n",
    "\n",
    "Let's have a look at a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# crosstab of survived against sex\n",
    "pd.crosstab(titanic['survived'],titanic['sex'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic use of `.crosstab()` requires that you input the row category ('survived') and the column category ('sex'). This produces cell counts for the cross-tabulation of the two categories, so there were 339 passengers who were both 'female' and 'survived'.\n",
    "\n",
    "The basic crosstab can be augmented with some additional parameters:\n",
    "* `margins` - set to `True` to add row and column totals.\n",
    "* `normalize` - create row or column proportions, or overall proportions, rather than frequencies. Keywords are 'index', 'columns' or 'all'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# crosstab of survived against sex\n",
    "pd.crosstab(titanic['survived'],titanic['sex'], margins=True, normalize = 'columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6\n",
    "\n",
    "1. What is the average fare paid by men and women?\n",
    "2. What is the median fare paid by men and women in each different class?\n",
    "3. Create a crosstab for 'survived' and 'pclass', which class is the best for survival?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# type your code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 6 solutions\n",
    "%load ../Solutions/Intro/exercise6.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<b>Section 8:</b> \n",
    "\n",
    "# Merging Data\n",
    "\n",
    "Often all the data you need to answer a question are not contained within a single dataset, but across several. Datasets can be joined, or 'merged', to allow data to be analysed together, but only **if the two datasets share a common reference or identifier.**\n",
    "\n",
    "Linking data come in a number of forms, and are commonly refered to as 'indexical' data. Some examples include:\n",
    "\n",
    "* Your NHS number, allowing data linkage across the NHS for primary, secondary, tertiary care episodes and prescribing.\n",
    "* Any account number (e.g. banking, utilities, travel card, council tax etc.) can acts as a point of linkage between different sets of data.\n",
    "* Your email, phone number, social media handles etc.\n",
    "* Your address can also act as a spatial reference, linking you to your neighbourhood, local services etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Different ways to merge\n",
    "\n",
    "While you may have heard it called \"Join\" in other languages, particularly in database query languages, in pandas we use the `.merge()` function.\n",
    "\n",
    "Once you have established that two DataFrames share a reference that will permit a merge to be conducted, you may wish to further specify how the merge behaves with the `how` parameter.\n",
    "\n",
    "* Inner - Only rows with reference values that appear in both DataFrames are merged.\n",
    "* Left - All the data from the 'left' DataFrame is retained, and any rows that have matching references are merged from the 'right' DataFrame.\n",
    "* Right - All data from the right and anything that matches from the left. Effectively, the reverse of 'Left'.\n",
    "* Outer (Full) - all data from the left and right DataFrame is retained, matched up where possible.\n",
    "\n",
    "This can be easier to understand graphically:\n",
    "\n",
    "![joins](https://www.dofactory.com/Images/sql-joins.png)\n",
    "\n",
    "Lets read in some additional titanic data and have a look at them.\n",
    "\n",
    "The new dataset includes the passenger name and age, as well as the additional variables:\n",
    "* boat - lifeboat identifier\n",
    "* body - body identification number\n",
    "* home.dest - the passenger's home and destination in the form \"home / dest\" or just \"home\".\n",
    "\n",
    "The dataset is located in the 'Data' folder, it is an excel file called: 'titanic_more.xlsx'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in the titanic_more.xlsx using pandas.\n",
    "titanic_more = pd.read_excel('../Data/titanic_more.xlsx')\n",
    "titanic_more.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to merge the two tables we need to use a column which uniquely defines each passenger and is available in both DataFrames. At first glance, 'name' would appear to be a good candidate for this, however, remember there are a couple of passengers who have the same name as each other.\n",
    "\n",
    "We can explicitly check is a column uniquely identifies rows with the `Series.is_unique` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Name is not unique defined for each row in titanic\n",
    "titanic['name'].is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Name is not unique defined for each row in titanic_more\n",
    "titanic_more['name'].is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can create a field that will uniquely identify passengers in both datesets by generating a new variable that combines the 'name' and 'age' variables. This is because we happen to know the ages of the passengers who have the same name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create unique id base don name and age for titanic\n",
    "titanic['name_age_id'] = titanic['name'] + \" \" + titanic['age'].astype(str)\n",
    "# Check if the new variable is unique\n",
    "titanic['name_age_id'].is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create unique id base don name and age for titanic\n",
    "titanic_more['nameageid'] = titanic_more['name'] + \" \" + titanic_more['age'].astype(str)\n",
    "# Check if the new variable is unique\n",
    "titanic_more['nameageid'].is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Merge function\n",
    "\n",
    "\n",
    "Now that we have unique id fields in both titanic and titanic_more, we use them to merge the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge the titanic_more dataset with titanic\n",
    "titanic_merge = titanic.merge(titanic_more[['name_age_id','boat','body','home.dest']], on = 'name_age_id')\n",
    "titanic_merge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code cell above we merge the `titanic_more` DataFrame into `titanic`. We do this on the basis of the 'name_age_id' variable that we created.\n",
    "\n",
    "The default merge behaviour is 'inner' join, however in this particular case all behaviours ('inner', 'left', 'right','outer') resolve to the same outcome as both datasets include the same 1,309 passengers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note that pandas actually handles multiple columns directly for unique identification.\n",
    "titanic_merge = titanic.merge(titanic_more[['name','age','boat','body','home.dest']], on = ['name','age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7\n",
    "\n",
    "Load the revised dataset 'titanic_revised.xlsx' which only includes passengers which have a value for boat, body or home.dest and try merging this to titanic.\n",
    "\n",
    "1. Which types of merge perform as expected?\n",
    "    * Try the different `how` parameters: 'inner', 'outer', 'left', 'right'.\n",
    "2. How many values are observed for boat, body, and home.dest?\n",
    "    * i.e. How many values are non-missing/\n",
    "    * Hint: use `.count()`\n",
    "3. How many passengers in the dataset used lifeboat number 3? What proportion of them were female?\n",
    "    * Hint: think about datatypes.\n",
    "4. How many passengers record 'New York' or 'NY' somewhere in the 'home.dest' column?\n",
    "    * If you do a selection using `Series.str.contains()` you need to specify the parameter na=False.\n",
    "    * This sets missing values to `False` in the boolean filter and excludes them from the selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 7 solution\n",
    "%load ../Solutions/Intro/exercise7.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Vignettes\n",
    "\n",
    "There are two additional notebooks to explore in the notebooks folder.\n",
    "1. [Vignette - Visualisation](Vignette-Visualisation.ipynb) : explores the basics of making graphical plots using pandas objects and the matplotlib plotting library.\n",
    "2. [Vignette - Statistics](Vignette-Statistics.ipynb): explores some basic statistics, including linear regression using pandas objects and the statsmodels econometric statistics package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
